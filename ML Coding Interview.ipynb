{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a forward prop, back prop and gradient descent\n",
    "# Using numpy and pandas \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.52480162 0.90751461 0.08562624 0.8877006 ]\n",
      " [0.32352666 0.06406996 0.42215198 0.83664668]\n",
      " [0.15538677 0.85168924 0.09790202 0.60626888]\n",
      " [0.40625755 0.92009818 0.41776366 0.79395252]\n",
      " [0.04124228 0.50825002 0.69123443 0.5091974 ]\n",
      " [0.47092465 0.88435548 0.07281662 0.53565106]\n",
      " [0.58628394 0.72889282 0.77559055 0.08574807]\n",
      " [0.80145614 0.24664979 0.24786368 0.48439403]\n",
      " [0.29673186 0.01592657 0.42907099 0.30087965]\n",
      " [0.11790448 0.85025958 0.87179089 0.34793621]]\n",
      "[1 0 0 0 0 1 0 1 0 1 1 1 0 0 1 1 0 1 1 1]\n"
     ]
    }
   ],
   "source": [
    "data_size = 20\n",
    "input_data = np.random.rand(data_size,4)\n",
    "print(input)\n",
    "label = np.random.randint(0,2,size = data_size)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial prediction\n",
      "[[0.55947022]\n",
      " [0.56657808]\n",
      " [0.56987152]\n",
      " [0.56493328]\n",
      " [0.57477014]\n",
      " [0.57004909]\n",
      " [0.5743703 ]\n",
      " [0.57507578]\n",
      " [0.57230137]\n",
      " [0.57744128]\n",
      " [0.58022675]\n",
      " [0.56193241]\n",
      " [0.57052532]\n",
      " [0.58459734]\n",
      " [0.57413423]\n",
      " [0.56969226]\n",
      " [0.57549139]\n",
      " [0.57686635]\n",
      " [0.57310215]\n",
      " [0.57872297]]\n",
      "total cost = 13.788369\n",
      "total cost = 13.816603\n",
      "total cost = 13.889927\n",
      "total cost = 14.054894\n",
      "total cost = 14.310339\n",
      "total cost = 14.445261\n",
      "total cost = 13.981686\n",
      "total cost = 13.756552\n",
      "total cost = 13.713374\n",
      "total cost = 13.689112\n",
      "total cost = 13.662284\n",
      "total cost = 13.632448\n",
      "total cost = 13.598921\n",
      "total cost = 13.560748\n",
      "total cost = 13.516693\n",
      "total cost = 13.465268\n",
      "total cost = 13.404828\n",
      "total cost = 13.333743\n",
      "total cost = 13.250664\n",
      "total cost = 13.154830\n",
      "total cost = 13.046320\n",
      "total cost = 12.926158\n",
      "total cost = 12.796235\n",
      "total cost = 12.659086\n",
      "total cost = 12.517576\n",
      "total cost = 12.374588\n",
      "total cost = 12.232750\n",
      "total cost = 12.094246\n",
      "total cost = 11.960733\n",
      "total cost = 11.833339\n",
      "total cost = 11.712726\n",
      "total cost = 11.599175\n",
      "total cost = 11.492684\n",
      "total cost = 11.393054\n",
      "total cost = 11.299959\n",
      "total cost = 11.213001\n",
      "total cost = 11.131751\n",
      "total cost = 11.055772\n",
      "total cost = 10.984639\n",
      "total cost = 10.917950\n",
      "total cost = 10.855329\n",
      "total cost = 10.796431\n",
      "total cost = 10.740940\n",
      "total cost = 10.688570\n",
      "total cost = 10.639064\n",
      "total cost = 10.592189\n",
      "total cost = 10.547736\n",
      "total cost = 10.505517\n",
      "total cost = 10.465365\n",
      "weight\n",
      "(4, 1)\n",
      "(1, 1)\n",
      "label\n",
      "[1 0 0 0 0 1 0 1 0 1 1 1 0 0 1 1 0 1 1 1]\n",
      "output\n",
      "[[0.98647345]\n",
      " [0.51835399]\n",
      " [0.5643076 ]\n",
      " [0.58525859]\n",
      " [0.50319043]\n",
      " [0.6558774 ]\n",
      " [0.57578104]\n",
      " [0.50605782]\n",
      " [0.6562828 ]\n",
      " [0.52940324]\n",
      " [0.93760751]\n",
      " [0.92905896]\n",
      " [0.54523487]\n",
      " [0.52276595]\n",
      " [0.53872079]\n",
      " [0.97653927]\n",
      " [0.55346602]\n",
      " [0.8672911 ]\n",
      " [0.91600337]\n",
      " [0.73932523]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class gradientDescent():\n",
    "    \n",
    "    def __init__(self, num_layer, grad_step, input_data):\n",
    "        self.grad_step = grad_step\n",
    "        self.num_iter = num_iter\n",
    "        self.input_data = input_data\n",
    "        input_features = input_data.shape[1]\n",
    "        self.w1 = np.random.rand(input_features, num_layer) \n",
    "        self.w2 = np.random.rand(num_layer, 1)\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        sig = 1/(1+ np.exp(-z))\n",
    "        return sig\n",
    "\n",
    "    def forwardProp(self, input_mat):\n",
    "        # output= input %*% weight\n",
    "        # w1 = 4x2, w2 = 2x1\n",
    "        z1 = np.matmul(input_mat, self.w1) # 10 x 2\n",
    "        self.h1 = self.sigmoid(z1) # 10 x 2\n",
    "        # layer %*% weight\n",
    "        # activation, sigmoid \n",
    "        # 1/(1+ exp(-x))\n",
    "        z2 = np.matmul(self.h1, self.w2)\n",
    "        yhat = sigmoid(z2)  # \n",
    "        return yhat\n",
    "\n",
    "\n",
    "    def backProp(self, label , output, input_mat):\n",
    "        # backprop is equal to \n",
    "        # h = label - prediction\n",
    "        # derivative dJ/dz2, where yhat = sigmoid(z2) \n",
    "        d1 = output - label # 10 x 1 \n",
    "\n",
    "        # derivative dJ/dw2\n",
    "        w2_grad = np.matmul(self.h1.T, d1)  # 2 x 1\n",
    "\n",
    "        # derivative dJ/dw1\n",
    "        d2 = np.matmul(d1, self.w2.T) # 10 x 2\n",
    "        \n",
    "        h1_deriv = self.h1*(1-self.h1)\n",
    "        d3 = d2 * h1_deriv  # 10 x 2 \n",
    "        w1_grad = np.matmul(input_mat.T, d3) # 4 x 2 \n",
    "        return w1_grad, w2_grad\n",
    "\n",
    "\n",
    "    def calcCost(self, label, output):\n",
    "        # calculate the cost \n",
    "        ce = label*np.log(output) + (1-label)*np.log(1- output)\n",
    "        cost = np.sum(-ce)\n",
    "        print('total cost = %f' % cost)\n",
    "\n",
    "    def forwardBackProp(self, label, input_mat):\n",
    "        output = self.forwardProp(input_mat)\n",
    "        calcCost(label, output)\n",
    "        w1_grad, w2_grad = self.backProp(label, output, input_mat)\n",
    "        self.w1+= - w1_grad* self.grad_step \n",
    "        self.w2+= - w2_grad* self.grad_step \n",
    "        return output\n",
    "\n",
    "\n",
    "    def iterForwardBackProp(self, label, num_iter):\n",
    "        self.num_iter = num_iter\n",
    "        self.label = np.expand_dims(label, axis=1)\n",
    "        print('initial prediction')\n",
    "        print(self.forwardProp(self.input_data))\n",
    "        for i in range(1, self.num_iter):\n",
    "            output = self.forwardBackProp(self.label, self.input_data)\n",
    "        return self.w1, self.w2, output\n",
    "\n",
    "\n",
    "num_layer = 1\n",
    "grad_step = 1\n",
    "num_iter = 50\n",
    "\n",
    "grad = gradientDescent(num_layer = num_layer, grad_step= grad_step, input_data= input_data)\n",
    "w1, w2, output = grad.iterForwardBackProp(label, num_iter)\n",
    "print('weight')\n",
    "print(w1.shape)\n",
    "print(w2.shape)\n",
    "print('label')\n",
    "print(label)\n",
    "print('output')\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial prediction\n",
      "[[0.58762534]\n",
      " [0.66831247]\n",
      " [0.68900606]\n",
      " [0.64438487]\n",
      " [0.7766326 ]\n",
      " [0.70176528]\n",
      " [0.72197841]\n",
      " [0.77556508]\n",
      " [0.73597212]\n",
      " [0.78326323]\n",
      " [0.8066096 ]\n",
      " [0.63595025]\n",
      " [0.72149169]\n",
      " [0.83279933]\n",
      " [0.72582709]\n",
      " [0.6781753 ]\n",
      " [0.75739709]\n",
      " [0.7692368 ]\n",
      " [0.73710098]\n",
      " [0.77050864]]\n",
      "total cost = 15.488048\n",
      "total cost = 14.386510\n",
      "total cost = 13.924371\n",
      "total cost = 13.696432\n",
      "total cost = 13.540483\n",
      "total cost = 13.407103\n",
      "total cost = 13.282855\n",
      "total cost = 13.164168\n",
      "total cost = 13.050018\n",
      "total cost = 12.940026\n",
      "total cost = 12.833979\n",
      "total cost = 12.731710\n",
      "total cost = 12.633065\n",
      "total cost = 12.537902\n",
      "total cost = 12.446082\n",
      "total cost = 12.357473\n",
      "total cost = 12.271947\n",
      "total cost = 12.189382\n",
      "total cost = 12.109662\n",
      "total cost = 12.032673\n",
      "total cost = 11.958307\n",
      "total cost = 11.886462\n",
      "total cost = 11.817037\n",
      "total cost = 11.749937\n",
      "total cost = 11.685072\n",
      "total cost = 11.622353\n",
      "total cost = 11.561698\n",
      "total cost = 11.503025\n",
      "total cost = 11.446259\n",
      "total cost = 11.391324\n",
      "total cost = 11.338152\n",
      "total cost = 11.286675\n",
      "total cost = 11.236827\n",
      "total cost = 11.188547\n",
      "total cost = 11.141776\n",
      "total cost = 11.096457\n",
      "total cost = 11.052536\n",
      "total cost = 11.009960\n",
      "total cost = 10.968680\n",
      "total cost = 10.928647\n",
      "total cost = 10.889817\n",
      "total cost = 10.852145\n",
      "total cost = 10.815589\n",
      "total cost = 10.780109\n",
      "total cost = 10.745667\n",
      "total cost = 10.712224\n",
      "total cost = 10.679747\n",
      "total cost = 10.648200\n",
      "total cost = 10.617551\n",
      "[[ 1.94957165]\n",
      " [-2.46514574]\n",
      " [ 1.95345955]\n",
      " [-0.50430124]]\n",
      "[1 0 0 0 0 1 0 1 0 1 1 1 0 0 1 1 0 1 1 1]\n",
      "[[0.78093247]\n",
      " [0.32332302]\n",
      " [0.42106942]\n",
      " [0.42021708]\n",
      " [0.35874696]\n",
      " [0.55085643]\n",
      " [0.41513141]\n",
      " [0.41054118]\n",
      " [0.62727359]\n",
      " [0.52908165]\n",
      " [0.84411339]\n",
      " [0.71920434]\n",
      " [0.46627414]\n",
      " [0.4754998 ]\n",
      " [0.37735256]\n",
      " [0.77110918]\n",
      " [0.53458693]\n",
      " [0.76974596]\n",
      " [0.75542468]\n",
      " [0.61661809]]\n"
     ]
    }
   ],
   "source": [
    "# Logistic Regression \n",
    "\n",
    "class LogisticRegression():\n",
    "    \n",
    "    def __init__(self,  grad_step):\n",
    "        self.grad_step = grad_step\n",
    "        \n",
    "    def sigmoid(self, z):\n",
    "        sig = 1/(1+ np.exp(-z))\n",
    "        return sig\n",
    "\n",
    "    def forwardProp(self, input_mat):\n",
    "        # output= input %*% weight\n",
    "        # w1 = 4x2, w2 = 2x1\n",
    "        z1 = np.matmul(input_mat, self.w1) # 10 x 2\n",
    "        yhat = self.sigmoid(z1) # 10 x 2\n",
    "        # layer %*% weight\n",
    "        # activation, sigmoid \n",
    "        # 1/(1+ exp(-x))\n",
    "        return yhat\n",
    "\n",
    "\n",
    "    def backProp(self, label , output, input_mat):\n",
    "        # backprop is equal to \n",
    "        # h = label - prediction\n",
    "        # derivative dJ/dz2, where yhat = sigmoid(z2) \n",
    "        d1 = output - label # 10 x 1 \n",
    "\n",
    "        # derivative dJ/dw2\n",
    "        w1_grad = np.matmul(input_mat.T, d1) # 4 x 2 \n",
    "        return w1_grad\n",
    "\n",
    "\n",
    "    def calcCost(self, label, output):\n",
    "        # calculate the cost \n",
    "        ce = label*np.log(output) + (1-label)*np.log(1- output)\n",
    "        cost = np.sum(-ce)\n",
    "        print('total cost = %f' % cost)\n",
    "\n",
    "    def forwardBackProp(self, label, input_mat):\n",
    "        output = self.forwardProp(input_mat)\n",
    "        calcCost(label, output)\n",
    "        w1_grad = self.backProp(label, output, input_mat)\n",
    "        self.w1+= - w1_grad* self.grad_step \n",
    "        return output\n",
    "\n",
    "\n",
    "    def iterForwardBackProp(self, label, input_data, num_iter):\n",
    "        self.num_iter = num_iter\n",
    "        input_features = input_data.shape[1]\n",
    "        self.w1 = np.random.rand(input_features, 1) \n",
    "        self.label = np.expand_dims(label, axis=1)\n",
    "        print('initial prediction')\n",
    "        print(self.forwardProp(input_data))\n",
    "        \n",
    "        for i in range(1, self.num_iter):\n",
    "            output = self.forwardBackProp(self.label, input_data)\n",
    "        return self.w1, output\n",
    "\n",
    "\n",
    "grad_step =.1\n",
    "logit = LogisticRegression(grad_step= grad_step)\n",
    "w1, output = logit.iterForwardBackProp(label, input_data, num_iter)\n",
    "\n",
    "print(w1)\n",
    "print(label)\n",
    "print(output)\n",
    "# create the algo for logistic regression "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
